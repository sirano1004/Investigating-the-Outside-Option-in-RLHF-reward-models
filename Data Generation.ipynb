{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.models import get_model, get_tokenizer, reward_model\n",
    "from utils.data_loader import get_data\n",
    "from utils.reward_scoring import generate_output, get_rewards\n",
    "from utils.preference_generation import determine_preference\n",
    "from peft import PeftModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv #Load HuggingFace Token\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CONFIG\n",
    "batch_size = 100\n",
    "for_sft = False\n",
    "for_dpo = True\n",
    "prompt_length = 20\n",
    "max_length = 196\n",
    "# Get Model\n",
    "model_name = 'google/gemma-3-270m'\n",
    "\n",
    "base_model = get_model(model_name).to('cuda')\n",
    "base_model = PeftModel.from_pretrained(base_model,\n",
    "                                      'models/sft/best_model')\n",
    "tok = get_tokenizer(model_name)\n",
    "\n",
    "# Get data\n",
    "base_data = get_data('train', 000, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_responses = []\n",
    "second_responses = []\n",
    "first_response_scores = []\n",
    "second_response_scores = []\n",
    "# Loop from 0 to the end of the list, jumping by batch_size each time\n",
    "for i in range(0, len(base_data), batch_size):\n",
    "    # Slice the list to get the current batch\n",
    "    batch = base_data[i : i + batch_size]\n",
    "\n",
    "    print(f\"Processing batch starting at index {i}: {batch[0][:20]}\")\n",
    "    first_response = generate_output(base_model, tok, batch, prompt_length, max_length)\n",
    "    second_response = generate_output(base_model, tok, batch, prompt_length, max_length)\n",
    "    first_responses.extend(first_response)\n",
    "    second_responses.extend(second_response)\n",
    "    first_response_scores.extend(get_rewards(reward_model, first_response))\n",
    "    second_response_scores.extend(get_rewards(reward_model, second_response))\n",
    "\n",
    "\n",
    "# Create a DataFrame from the lists\n",
    "df = pd.DataFrame({\n",
    "    'first_response': first_responses,\n",
    "    'second_response': second_responses,\n",
    "    'first_response_score': first_response_scores,\n",
    "    'second_response_score': second_response_scores\n",
    "})\n",
    "\n",
    "# Apply the function to create the 'pref' column\n",
    "df['pref'] = df.apply(determine_preference, axis=1)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "display(df.head())\n",
    "df.to_csv('data/dpo_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
